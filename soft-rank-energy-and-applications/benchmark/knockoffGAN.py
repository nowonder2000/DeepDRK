# code was taken from https://bitbucket.org/mvdschaar/mlforhealthlabpub/src/master/

import numpy as np
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

#initialization
def xavier_init(size):
    '''
    sampling normal distribution with standard deviation 1/sqrt(n/2)
    '''
    in_dim = size[0]
    xavier_stddev = 1./tf.sqrt(in_dim/2)
    return tf.random_normal(shape = size, stddev = xavier_stddev)

# random variable generation: sample from normal distribution
def sample_Z(m, n, x_name):
    if ((x_name == 'Normal') | (x_name == 'AR_Normal')):
        return np.random.normal(0., np.sqrt(1./3000), size = [m, n]).copy()
    elif ((x_name == 'Uniform') | (x_name == 'AR_Uniform')):
        return np.random.uniform(-3*np.sqrt(1./3000),3*np.sqrt(1./3000),[m,n]).copy()

#Samole from the real data(MiniBatch index sampling)
def sample_X(m, n):
    return np.random.permutation(m)[:n].copy()

# permutation for MINE computation
def Permute(x):
    d = x.shape[0]
    idx = np.random.permutation(d)
    out = x[idx, :].copy()
    return out

# Bernoulli sampling for swap and hint variables
def sample_SH(m, n,p):
    return np.random.binomial(1, p, [m,n]).copy()

def knockoffgan(xTrain, xTest, distType = 'MultivariateStudentT'):
    X = xTrain
    n, x_dim = X.shape
    x_train = X
    x_name = "Normal"
    z_dim, h_dim = x_dim, x_dim
    mb_size = 128 # 128
    lr = 1e-4
    
    # 1. Feature
    X = tf.placeholder(tf.float32, shape = [None, x_dim])  
    # 2. Feature (Permute)
    X_hat = tf.placeholder(tf.float32, shape = [None, x_dim])   
    # 3. Random Variable    
    Z = tf.placeholder(tf.float32, shape = [None, z_dim])
    # 4. Swap
    S = tf.placeholder(tf.float32, shape = [None, x_dim])   
    # 5. Hint
    H = tf.placeholder(tf.float32, shape = [None, x_dim])

        #%% Network Building
    
    # Input: Swap(X, X_tilde) and Hint
    D_W1 = tf.Variable(xavier_init([x_dim + x_dim + x_dim, h_dim]))
    D_b1 = tf.Variable(tf.zeros(shape = [h_dim]))
    
    D_W2 = tf.Variable(xavier_init([h_dim, x_dim]))
    D_b2 = tf.Variable(tf.zeros(shape =[x_dim]))
    theta_D = [D_W1, D_W2, D_b1, D_b2]
    
    #%% 2. WGAN Discriminator
    # Input: tilde X
    WD_W1 = tf.Variable(xavier_init([x_dim, h_dim]))
    WD_b1 = tf.Variable(tf.zeros(shape=[h_dim]))
    
    WD_W2 = tf.Variable(xavier_init([h_dim,1]))
    WD_b2 = tf.Variable(tf.zeros(shape=[1]))
    theta_WD = [WD_W1, WD_W2, WD_b1, WD_b2]
    
    #%% 3. Generator
    # Input: X and Z
    G_W1 = tf.Variable(xavier_init([x_dim + z_dim, h_dim]))
    G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))
    
    G_W2 = tf.Variable(xavier_init([h_dim,x_dim]))
    G_b2 = tf.Variable(tf.zeros(shape=[x_dim]))
    
    theta_G = [G_W1, G_W2, G_b1, G_b2]
    
    #%% 4. MINE
    # Input: X and tilde X
    # For X    
    M_W1A = tf.Variable(xavier_init([x_dim]))
    M_W1B = tf.Variable(xavier_init([x_dim]))
    M_b1 = tf.Variable(tf.zeros(shape=[x_dim]))
    
    # For tilde X
    M_W2A = tf.Variable(xavier_init([x_dim]))
    M_W2B = tf.Variable(xavier_init([x_dim]))
    M_b2 = tf.Variable(tf.zeros(shape=[x_dim]))
    
    # Combine
    M_W3 = tf.Variable(xavier_init([x_dim]))
    M_b3 = tf.Variable(tf.zeros(shape=[x_dim]))
    
    theta_M = [M_W1A, M_W1B, M_W2A, M_W2B, M_W3, M_b1, M_b2, M_b3]
    # 1. Generator
    def generator(x, z):
        inputs = tf.concat(axis =1, values =[x, z])
        G_h1 = tf.nn.tanh(tf.matmul(inputs, G_W1) + G_b1)
        G_out = tf.matmul(G_h1, G_W2) + G_b2
        return G_out
    
    # 2.Discrinimator
    def discriminator(sA, sB, h):
        inputs = tf.concat(axis=1, values = [sA, sB, h])
        D_h1 = tf.nn.tanh(tf.matmul(inputs, D_W1) + D_b1)
        D_out = tf.nn.sigmoid(tf.matmul(D_h1, D_W2) + D_b2)
    
        return D_out
        
    # 3. WGAN Discriminator    
    def WGAN_discriminator(x):
        WD_h1 = tf.nn.relu(tf.matmul(x, WD_W1) + WD_b1)
        WD_out = (tf.matmul(WD_h1, WD_W2) + WD_b2)
    
        return WD_out        
    
    # 4. MINE   
    def MINE(x, x_hat):
        M_h1 = tf.nn.tanh(M_W1A * x + M_W1B * x_hat + M_b1)
        M_h2 = tf.nn.tanh(M_W2A * x + M_W2B * x_hat + M_b2)
        M_out = (M_W3 * (M_h1 + M_h2) + M_b3)
    
        Exp_M_out = tf.exp(M_out)    
    
        return M_out, Exp_M_out 
    lamda = 0.01
    lam = 10
    mu = 1
    mb_size=128
    niter=5000
    
    
    # In[13]:
    
    
    #%% Combination of networks
    # 1. Generator knockoffs
    G_sample = generator(X,Z)
    
    # 2. WGAN Outputs for real and fake
    WD_real = WGAN_discriminator(X)
    WD_fake = WGAN_discriminator(G_sample)
    
    # 3. Generate swapping (X, tilde X)
    SwapA = S * X + (1-S) * G_sample
    SwapB = (1-S) * X + S * G_sample
    
    # 4. Discriminator output 
    # (X, tilde X) is SwapA, SwapB. Hint is generated by H * S 
    D_out = discriminator(SwapA, SwapB, H*S)    
    
    # 5. MINE Computation
    # Without permutation
    M_out, _ = MINE(X, G_sample)
    # Wit permutation
    _, Exp_M_out = MINE(X_hat, G_sample)
    
    # 6. WGAN Loss Replacement of Clipping algorithm to Penalty term
    # 1. Line 6 in Algorithm 1
    eps = tf.random_uniform([mb_size, 1], minval = 0., maxval = 1.)
    X_inter = eps*X + (1. - eps) * G_sample
    
    # 2. Line 7 in Algorithm 1
    grad = tf.gradients(WGAN_discriminator(X_inter), [X_inter])[0]
    grad_norm = tf.sqrt(tf.reduce_sum((grad)**2 + 1e-8, axis = 1))
    grad_pen = lam * tf.reduce_mean((grad_norm - 1)**2)
    
    
    # In[14]:
    
    
    #%% Loss function
    # 1. WGAN Loss
    WD_loss = tf.reduce_mean(WD_fake) - tf.reduce_mean(WD_real) + grad_pen
    
    # 2. Discriminator loss
    D_loss = -tf.reduce_mean(S * (1-H) * tf.log(D_out + 1e-8) + (1-S) * (1-H) * tf.log(1 - D_out + 1e-8))   
    
    # 3. MINE Loss
    M_loss = tf.reduce_sum( tf.reduce_mean(M_out, axis = 0) - tf.log(tf.reduce_mean(Exp_M_out, axis = 0)) )
    
    # 4. Generator loss
    G_loss =  - D_loss + mu * -tf.reduce_mean(WD_fake) + lamda * M_loss
    
    # Solver
    WD_solver = (tf.train.AdamOptimizer(learning_rate = lr, beta1 = 0.5).minimize(WD_loss, var_list = theta_WD))
    D_solver = (tf.train.AdamOptimizer(learning_rate = lr, beta1 = 0.5).minimize(D_loss, var_list = theta_D))
    G_solver = (tf.train.AdamOptimizer(learning_rate = lr, beta1 = 0.5).minimize(G_loss, var_list = theta_G))
    M_solver = (tf.train.AdamOptimizer(learning_rate = lr, beta1 = 0.5).minimize(-M_loss, var_list = theta_M))
    
    
    # In[15]:
    
    config = tf.ConfigProto()
    config.log_device_placement=True
    config.gpu_options.allow_growth = True
    sess = tf.Session(config=config)
    sess.run(tf.global_variables_initializer())
    run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)
    
    
    
    # In[16]:
    
    
    for it in (range(niter)):
        check_mine = []
        for dummy_range in range(5):
            #%% WGAN, Discriminator and MINE Training    
    
            # Random variable generation
            Z_mb = sample_Z(mb_size, z_dim, x_name)            
    
            # Minibatch sampling
            X_idx = sample_X(n,mb_size)        
            X_mb = x_train[X_idx,:].copy()
            X_perm_mb = Permute(X_mb)
    
            # Swap generation
            S_mb = sample_SH(mb_size, x_dim, 0.5)
    
            # Hint generation
            H_mb = sample_SH(mb_size, x_dim, 0.9)
    
            # 1. WGAN Training
            _, WD_loss_curr = sess.run([WD_solver, WD_loss], feed_dict = {X: X_mb, Z: Z_mb, X_hat: X_perm_mb, S: S_mb, H: H_mb}, options=run_opts)
    
            # 2. Discriminator Training
            # print('discriminator training')
            _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict = {X: X_mb, Z: Z_mb, X_hat: X_perm_mb, S: S_mb, H: H_mb}, options=run_opts)
    
            # 3. MINE Training
            # print('mine training')
            _, M_loss_curr = sess.run([M_solver, M_loss], feed_dict = {X: X_mb, Z: Z_mb, X_hat: X_perm_mb, S: S_mb, H: H_mb}, options=run_opts)   
            check_mine.append(M_loss_curr)
        print(f'Iter: {it}, MINE: {np.mean(check_mine)}')
        #%% Generator Training
    
        # Random variable generation                
        Z_mb = sample_Z(mb_size, z_dim, x_name)             
    
        # Minibatch sampling
        X_idx = sample_X(n,mb_size)        
        X_mb = x_train[X_idx,:].copy()
        X_perm_mb = Permute(X_mb)
    
        # Swap generation
        S_mb = sample_SH(mb_size, x_dim, 0.5)
    
        # Hint generation
        H_mb = sample_SH(mb_size, x_dim, 0.0)
    
        # Generator training
        #print('gen training')
        _, G_loss_curr, G_sample_curr = sess.run([G_solver, G_loss, G_sample],
                                                 feed_dict = {X: X_mb, Z: Z_mb, X_hat: X_perm_mb, S: S_mb,
                                                              H: H_mb}, options=run_opts)

    
    
#     gen_fn = lambda x: sess.run([G_sample], feed_dict = {X: x, Z: sample_Z(x.shape[0], z_dim, x_name)}, 
#                            options=run_opts)[0]
    
    def gen_all(xTest):
        xTestGan = list()
        for i in range(len(xTest)):
            #print(type(xTest[i]), xTest[i].shape, sample_Z(xTest[i].shape[0], z_dim, x_name).shape)
            tmp_val = xTest[i][None, ...]
            xTestGan.append(sess.run([G_sample], feed_dict = {X: tmp_val, Z: sample_Z(tmp_val.shape[0], z_dim, x_name)}, 
                                   options=run_opts)[0])
        return xTestGan
        
    xTestGan = gen_all(xTest)
    return xTestGan, gen_all
